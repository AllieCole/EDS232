---
title: "EDS232_lab2"
output:
  html_document:
    code_folding: hide
---

```{r setup, include= FALSE, message= FALSE, warning= FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1 Clustering
Clustering associates similar data points with each other, adding a grouping label. It is a form of unsupervised learning since we don’t fit the model based on feeding it a labeled response (i.e. y).

## 1.1 K-Means Clustering
Source: K Means Clustering in R | DataScience+

In k-means clustering, the number of clusters needs to be specified. The algorithm randomly assigns each observation to a cluster, and finds the centroid of each cluster. Then, the algorithm iterates through two steps:

Reassign data points to the cluster whose centroid is closest.
Calculate new centroid of each cluster.
These two steps are repeated until the within cluster variation cannot be reduced any further. The within cluster variation is calculated as the sum of the euclidean distance between the data points and their respective cluster centroids.

### 1.1.1 Load and plot the penguins dataset
The penguins dataset comes from Allison Horst’s palmerpenguins R package and records biometric measurements of different penguin species found at Palmer Station, Antarctica (Gorman, Williams, and Fraser 2014). It is an alternative to the iris dataset example for exploratory data analysis (to avoid association of this 1935 dataset’s collector Ronald Fisher who “held strong views on race and eugenics”). Use of either dataset will be acceptable for submission of this lab (and mention of iris or Fisher will be dropped for next year).


```{r}
# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, palmerpenguins, skimr, tibble)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("penguins")

# look at documentation in RStudio
if (interactive())
  help(penguins)

# show data table
datatable(penguins)
skim(penguins)


```
Table 1: Data summary
Name	penguins
Number of rows	344
Number of columns	8
_______________________	
Column type frequency:	
factor	3
numeric	5
________________________	
Group variables	None
Variable type: factor

skim_variable	n_missing	complete_rate	ordered	n_unique	top_counts
species	0	1.00	FALSE	3	Ade: 152, Gen: 124, Chi: 68
island	0	1.00	FALSE	3	Bis: 168, Dre: 124, Tor: 52
sex	11	0.97	FALSE	2	mal: 168, fem: 165
Variable type: numeric

skim_variable	n_missing	complete_rate	mean	sd	p0	p25	p50	p75	p100	hist
bill_length_mm	2	0.99	43.92	5.46	32.1	39.23	44.45	48.5	59.6	▃▇▇▆▁
bill_depth_mm	2	0.99	17.15	1.97	13.1	15.60	17.30	18.7	21.5	▅▅▇▇▂
flipper_length_mm	2	0.99	200.92	14.06	172.0	190.00	197.00	213.0	231.0	▂▇▃▅▂
body_mass_g	2	0.99	4201.75	801.95	2700.0	3550.00	4050.00	4750.0	6300.0	▃▇▆▃▂
year	0	1.00	2008.03	0.82	2007.0	2007.00	2008.00	2009.0	2009.0	▇▁▇▁▇


```{r}
# remove the rows with NAs
penguins <- na.omit(penguins)

# plot petal length vs width, species naive
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()

# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos

```

### 1.1.2 Cluster penguins using kmeans()


```{r}
# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# show cluster result
penguins_k


```


```{r}
# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)

```


```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos

```
**Question: Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.**

When using the kmeans() cluster plot, group 1 is most similar to the chinstrap and gentoo groups, but they are split in half. And the Adelie group is most similar to group two. The kmeans() cluster plot looks liek ther eis much more vertical seperation meaning it seperates the clusters along the bill_length measurement more than the bill_depth. 


### 1.1.3 Plot Voronoi diagram of clustered penguins
This form of clustering assigns points to the cluster based on nearest centroid. You can see the breaks more clearly with a Voronoi diagram.


```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")

```

**Task: Show the Voronoi diagram for fewer (k=2) and more (k=8) clusters to see how assignment to cluster centroids work.**

```{r}
# cluster using kmeans
k <- 2  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)
# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))
# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```


```{r}
# cluster using kmeans
k <- 8  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)
# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))
# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```


## 1.2 Hierarchical Clustering
Next, you’ll cluster sites according to species composition. You’ll use the dune dataset from the vegan R package.

### 1.2.1 Load dune dataset

```{r}

librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)

```

**Question: What are the rows and columns composed of in the dune data frame?**
The dune meadow vegetation data, dune, has cover class values of 30 species on 20 sites at differnt Dutch Dune Meadows.


### 1.2.2 Calculate Ecological Distances on sites
Before we calculate ecological distance between sites for dune, let’s look at these metrics with a simpler dataset, like the example given in Chapter 8 by Kindt and Coe (2005).


```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites

```


```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan

```


```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean

```

```{r} 
sites_bray <- vegdist(sites, method="bray")
sites_bray

```
**Question: In your own words, how does Bray Curtis differ from Euclidean distance? See sites_euclidean versus sites_bray from lab code, slides from Lecture 05. Clustering and reading Chapter 8 of Kindt and Coe (2005).**

 The Euclidean Distance measures the distance between 2 points and takes into account the abundance of each species. While the Bray curtis is only values range from 0 to 1 (zero mots and 1 least similar) and is a dissimilarity based on the sum of lowest counts of shared species between sites over the sum of all species. 

      
### 1.2.3 Bray-Curtis Dissimilarity on sites
Let’s take a closer look at the Bray-Curtis Dissimilarity distance:

Bij=1−2CijSi+Sj

Bij: Bray-Curtis dissimilarity value between sites i and j.
1 = completely dissimilar (no shared species); 0 = identical.

Cij: sum of the lesser counts C for shared species common to both sites i and j

SiORj: sum of all species counts S for the given site i or j

So to calculate Bray-Curtis for the example sites:

BAB=1−2∗(1+1)2+10=1−4/12=1−1/3=0.667

BAC=1−2∗02+1=1

BBC=1−2∗010+1=1

### 1.2.4 Agglomerative hierarchical clustering on dune
See text to accompany code: HOMLR 21.3.1 Agglomerative hierarchical clustering.


```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
NULL
as.matrix(d)[1:5, 1:5]

```


```{r}      
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)

```
**Question: Which function comes first, vegdist() or hclust(), and why? See HOMLR 21.3.1 Agglomerative hierarchical clustering.**
vegdist() comes first so you can make a matrix with a specified distance method for the hclust() function to use.

**Question:** In your own words, how does hclust() differ from agnes()? See HOMLR 21.3.1 Agglomerative hierarchical clustering and help documentation (?hclust(), ?agnes()).

I think that they are pretty simaler, agnes() generally does the same things as hclust() however, also can give you the agglomeration coefficient (AC). The AC is useful in describing the strength of the clustering structure.

**Question:** Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?

- The “ward” method has the highest AC and so gives the strongest clustering structure

```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac

```


```{r}
# Dendrogram plot of hc2
plot(hc2, which.plot = 2)

# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)

```  


```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac


```


```{r}
# Dendrogram plot of hc3
plot(hc3, which.plot = 2)

```

### 1.2.5 Divisive hierarchical clustering on dune
See text to accompany code: HOMLR 21.3.2 Divisive hierarchical clustering.


```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc


```
**Question: In your own words how does agnes() differ from diana()? See HOMLR 21.3.1 Agglomerative hierarchical clustering, slides from Lecture 05. Clustering and help documentation (?agnes(), ?diana()).**

agnes() does the agglomerative hierarchical clustering, which is the leaves to root method and we get an agglomerative coefficient. 
The diana() performs divisive hierarchical clustering, which is the root to leaves method, and we get a divisive coefficient


### 1.2.6 Determining optimal clusters
See text to accompany code: HOMLR 21.4 Determining optimal clusters.


```{r}
librarian::shelf(factoextra)

# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)


```

### 1.2.7 Working with dendrograms
See text to accompany code: HOMLR 21.5 Working with dendrograms.


```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
sub_grp


```
**Question: How do the optimal number of clusters compare between methods for those with a dashed line?**
- For the silhouette method the optimal number of clusters is 4, and for the gap method the optimal number of clusters is 3. those seem to be pretty close so im not sure which woudl be better


**Question: In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection? See HOMLR 21.5 Working with dendrograms.**

The height of their shared connection is the biggest determinant of relatedness between observations. Because the height shows when clusters were formed and therefore measures the distance between observations

```{r}
# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")

```

#2 Ordination
Ordination orders sites near each other based on similarity. It is a multivariate analysis technique used to effectively collapse dependent axes into fewer dimensions, i.e. dimensionality reduction. It also falls into the class of unsupervised learning because a “response term” is not used to fit the model.

##2.1 Principal Components Analysis (PCA)
Although this example uses a non-ecological dataset, it goes through the materials walk through the idea and procedure of conducting an ordination using the most widespread technique.

Please read the entirety of Chapter 17 Principal Components Analysis | Hands-On Machine Learning with R. Supporting text is mentioned below where code is run.

###2.1.1 Prerequisites
See supporting text: 17.1 Prerequisites

```{r}
# load R packages
librarian::shelf(
  dplyr, ggplot2, h2o)

# set seed for reproducible results
set.seed(42)

# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
```


```{r}
my_basket
```

### 2.1.2 Performing PCA in R
See supporting text: 17.4 Performing PCA in R

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance
```

R is connected to the H2O cluster: 
    H2O cluster uptime:         23 hours 50 minutes 
    H2O cluster timezone:       America/Los_Angeles 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.36.0.1 
    H2O cluster version age:    1 month and 1 day  
    H2O cluster name:           H2O_started_from_R_bbest_qea893 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   5.00 GB 
    H2O cluster total cores:    12 
    H2O cluster allowed cores:  12 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4 
    R Version:                  R version 4.1.1 (2021-08-10) 
    
```{r}    
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)
```
**Question: How many inital principal components are chosen with respect to dimensions of the input data? See HOMLR 17.4 Performing PCA in R.** 

There are `r ncol(my_basket.h2o)` inital principal components


```{r}
# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()

my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()

```
**Question: What category of grocery items contribute most to PC1? (These are related because they're bought most often together on a given grocery trip)**

Wine and fosters on teh right but also carrots and potatoes seemed to have a good effect on the left of PC1


**Question: Why is the pca_method of “GramSVD” chosen over “GLRM”? See HOMLR 17.4 Performing PCA in R.** 

The "GramSVD" method is used when you have mostly numeric variables, while the "GLRM" method is used when you have mostly categorical variables. This data is mostly numeric so GramSVD is used.



### 2.1.3 Eigenvalue criterion
See supporting text: 17.5.1 Eigenvalue criterion.

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
```
 
```{r}  
# Sum of all eigenvalues equals number of variables
sum(eigen)
```

```{r}
# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)

```

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")

```
**Question: What category of grocery items contribute the least to PC1 but positively towards PC2?**

- Vegetables, like carrost contribute the least to PC1 but positively to PC2

**Question: How many principal components would you include to explain 90% of the total variance?**

- About 36 PCs

```{r}
# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))

```

```{r}
# Screee plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)

```
**Question: How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?**

8 PCs

**Question: What are a couple of disadvantages to using PCA? See HOMLR 17.6 Final thoughts.**
PCA can be highly affected by outliers, and can get bogged down by complexities (just like me) in high demention space


## 2.2 Non-metric MultiDimensional Scaling (NMDS)
### 2.2.1 Unconstrained Ordination on Species
See supporting text: 2.1 Non-metric Multidimensional scaling in vegantutor.pdf:

```{r}  
# load R packages
librarian::shelf(
  vegan, vegan3d)

# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry
```

```{r}
varespec %>% tibble()
```
**Question: What are the dimensions of the varespec data frame and what do rows versus columns represent?**

The dimensions are 24 rows by 44 columns. The rows are Lichen Pasture sites with percent cover, and the columns are 44 differnet species

```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```
**Question: The “stress” in a stressplot represents the difference between the observed inpnut distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?**

- The \(R^2\)  for the NMDS is 0.99 and the \(R^2\)  for the linear fit is 0.94, so the non-metric fit is better than the linear fit by a difference of 0.05 in the \(R^2\) value.


```{r}
ordiplot(vare.mds0, type = "t")
```
**Question: What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?**

For MDS1 it is Sites 5 and 28 and its sites 14 and 21 for MDS2

```{r}
vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds

```

```{r}
plot(vare.mds, type = "t")

```
**Question: What is the basic difference between metaMDS and monoMDS()? See 2.1 Non-metric Multidimensional scaling of vegantutor.pdf.**

`metaMDS()` uses `monoMDS()` to create a nonlinear regression from multiple different random starts


### 2.2.2 Overlay with Environment
See supporting text in vegantutor.pdf:

3 Environmental interpretation
3.1 Vector fitting
3.2 Surface fitting

```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef
```

```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")
```
**Question: What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?**

Al and Fe

**Question: Which of the two NMDS axes differentiates Ca the most, i.e. has the highest value given by the contours at the end (and not middle) of the axis?**

NMDS1


### 2.2.3 Constrained Ordination on Species and Environment
See supporting text in vegantutor.pdf:

4 Constrained ordination
4.1 Model specification
Technically, this uses another technique cca, or canonical correspondence analysis.

```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```

```{r}
# plot ordination
plot(vare.cca)
```
**Question: What is the difference between “constrained” versus “unconstrained” ordination within ecological context?**

Within teh ecogogical context, constrained ordination includes two or more sets of environmental data and looks at the relationship between the different variables. Unconstrained ordination doesnt have any structure.


```{r}
# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
```

```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```

**Question: What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environmnent? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?**
Sites 28 and 4 are farthest apart on the x, so most differentiated by CCA1. The longest environmental vector is Al






